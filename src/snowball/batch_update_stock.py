import random
import time
from datetime import UTC, date, datetime
from typing import Optional, Tuple

import requests
from bs4 import BeautifulSoup

from src.database import SessionLocal
from src.snowball.models import Stock

# âœ… í¬ë¡¤ë§ ëŒ€ìƒ ì¢…ëª©
TICKERS = ["SPY", "QQQ", "GLD", "BIL"]

# âœ… User-Agent ì„¤ì •
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36"
}


# âœ… HTML ìš”ì²­ í•¨ìˆ˜
def fetch_html(url: str) -> Optional[BeautifulSoup]:
    """ì•¼í›„ íŒŒì´ë‚¸ìŠ¤ì—ì„œ HTML ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ BeautifulSoup ê°ì²´ë¡œ ë°˜í™˜"""
    try:
        time.sleep(random.uniform(1, 3))  # ëœë¤ ë”œë ˆì´ (429 ë°©ì§€)
        response = requests.get(url, headers=HEADERS)

        if response.status_code == 200:
            return BeautifulSoup(response.text, "html.parser")
        else:
            print(f"âš ï¸ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
            return None
    except requests.RequestException as e:
        print(f"âŒ ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


# âœ… HTMLì—ì„œ ìµœì‹  ì¢…ê°€ ë°ì´í„° íŒŒì‹± í•¨ìˆ˜
def parse_latest_stock_data(soup: BeautifulSoup) -> Optional[Tuple[date, float]]:
    """HTMLì—ì„œ ìµœì‹  ë‚ ì§œ ë° Adjusted Close ê°’ì„ ì¶”ì¶œ"""
    table_rows = soup.select("table tbody tr")

    if table_rows:
        latest_row = table_rows[0].find_all("td")

        if len(latest_row) >= 6:
            latest_date = latest_row[0].text.strip()
            adj_close = latest_row[5].text.strip()

            try:
                latest_date = datetime.strptime(latest_date, "%b %d, %Y").date()
                adj_close = float(adj_close.replace(",", ""))

                return latest_date, adj_close
            except ValueError as e:
                print(f"âŒ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
                return None
    return None


# âœ… ë°°ì¹˜ ì‹¤í–‰ í•¨ìˆ˜
def run_batch():
    print(f"ğŸ“Œ ETF ê°€ê²© ì—…ë°ì´íŠ¸ ì‹œì‘: {datetime.now(UTC)}")

    db = SessionLocal()

    try:
        for ticker in TICKERS:
            url = f"https://finance.yahoo.com/quote/{ticker}/history"
            soup = fetch_html(url)

            if not soup:
                print(f"âŒ {ticker} ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨, ìŠ¤í‚µ")
                break

            stock_data = parse_latest_stock_data(soup)

            if not stock_data:
                print(f"âŒ {ticker} Adjusted Close ê°’ íŒŒì‹± ì‹¤íŒ¨, ìŠ¤í‚µ")
                break

            latest_date, adj_close = stock_data
            stock_entry = Stock(date=latest_date, ticker=ticker, price=adj_close)
            db.merge(stock_entry)
            print(f"âœ… {ticker} ì €ì¥ ì™„ë£Œ: {latest_date} - ${adj_close}")

        db.commit()
        print("âœ… ëª¨ë“  ì¢…ëª© ì—…ë°ì´íŠ¸ ì™„ë£Œ.")

    except Exception as e:
        db.rollback()
        print(f"âŒ ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    finally:
        db.close()


if __name__ == "__main__":
    run_batch()
